# **************************************************************************** #
#                                                                              #
#                                                         :::      ::::::::    #
#    1_AI_Assistant_2.py                                :+:      :+:    :+:    #
#                                                     +:+ +:+         +:+      #
#    By: ammar syed ali <https://www.linkedin.co    +#+  +:+       +#+         #
#                                                 +#+#+#+#+#+   +#+            #
#    Created: 2023/11/05 02:33:51 by ammar syed        #+#    #+#              #
#    Updated: 2023/11/05 02:33:51 by ammar syed       ###   ########.fr        #
#                                                                              #
# **************************************************************************** #
"""

"""


import openai
import streamlit as st
import os
from dotenv import load_dotenv

# Get the path to the directory this file is in
CURRENT_DIR = os.path.abspath(os.path.dirname(__file__))

# Connect the path with your '.env' file name located one level above
BASEDIR = os.path.abspath(os.path.join(CURRENT_DIR, os.pardir))
load_dotenv(os.path.join(BASEDIR, '.env'))
openai.api_key = os.getenv('OPENAI_API_KEY')

st.title("AI Assistant 2: Remember the Conversation")

# The state object stores information as your app updates.
# Streamlit re-runs the entire script everytime a button is pressed like C# (I think)
# Set a default model

if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-3.5-turbo"

# State can store data in many forms. The messages list will be a dictionary list
# whera above the model is a key value pair
# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]): # st.chat_message creates each message based on the role
        st.markdown(message["content"]) # the code within the message block appears within the message

full_response = "" # this variable will append each chunk of text generated by OpenAiI

# Accept user input writing what is up if nothing is typed in the text widget.
if prompt := st.chat_input("What is up?"):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt}) # records the user's prompt
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)
    # Display assistant response in chat message container
    with st.chat_message("assistant"):
        message_placeholder = st.empty() 
        full_response = "" # this variable will append each chunk of text generated by OpenAiI
    # this for loop iterates over each response created by the openai.ChatCompletion object
    for response in openai.ChatCompletion.create(
        model=st.session_state["openai_model"],
        messages=[{"role": m["role"], "content": m["content"]} for m in st.session_state.messages],
        max_tokens=1024,
        n=1,
        stop=None,
        stream=True, # this argument makes the output come out in real time.
    ):
        full_response += response.choices[0].delta.get("content", "") #append each chunk of text onto the full response
        message_placeholder.markdown(full_response + "â–Œ") # print the response into the assistants message container
    message_placeholder.markdown(full_response)
st.session_state.messages.append({"role": "assistant", "content": full_response}) # record the assitant's response